{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDlnLNdEqIaX"
      },
      "source": [
        "# Предобработка текстовых данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiyiO57LqIaa"
      },
      "source": [
        "Работа с текстовыми данными обычно начинается с предобработки. Предобработка - это удаление всего лишнего и приведение к нужному формату. Предобработка в большинстве случаев все сводится к использованию стандартных инструментов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub56hiY5qIaa"
      },
      "source": [
        "Подготовка текста\n",
        "- токенизация (слова/n-граммы)\n",
        "- приведение к нижнему регистру\n",
        "- лемматизация (приведение слова к начальной/базовой форме - лемме)\n",
        "- стемминг (нахождение грамматической основы слова - корень слова. Не изменяемая часть при различных вариантах употребления слова)\n",
        "- удаление стоп-слов\n",
        "- удаление пунктуации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQw4GOmqqIab",
        "tags": []
      },
      "source": [
        "#### Установите все нужные библиотеки. Подробнее про каждую из них ниже"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N0qtlolqIab",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# морфологический анализатору Mystem от Яндекса.\n",
        "# Этот инструмент позволяет проводить морфологический\n",
        "# анализ русских слов (определение их формы, части речи и т. д.).\n",
        "!pip install pymystem3 -q\n",
        "\n",
        "# pymorphy2 предоставляет функционал для морфологического анализа русских слов.\n",
        "# Опция [fast] указывает на установку быстрой версии библиотеки.\n",
        "# !pip install pymorphy2[fast]\n",
        "\n",
        "# razdel предоставляет функции для токенизации\n",
        "# (разделения текста на отдельные токены, например, слова или предложения)\n",
        "# русскоязычных текстов.\n",
        "!pip install razdel -q\n",
        "\n",
        "# gensim предоставляет инструменты для моделирования темы\n",
        "# и векторного представления текста.\n",
        "!pip install gensim -q\n",
        "\n",
        "# nltk (Natural Language Toolkit) предоставляет множество инструментов\n",
        "# для работы с естественным языком, таких как токенизация,\n",
        "# стемминг, разметка речи и многое другое.\n",
        "!pip install nltk -q\n",
        "\n",
        "# rusenttokenize предоставляет функции для токенизации русскоязычных текстов\n",
        "# на предложения.\n",
        "!pip install rusenttokenize -q\n",
        "\n",
        "# регулярки\n",
        "!pip install regex -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIbyWY2pqIav"
      },
      "source": [
        "## Токенизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkXeL6xVqIav"
      },
      "source": [
        "Предложения нужно разбить на токены. Под токенами обычно понимаются слова, но это могут быть и какие-то более длинные или короткие куски.   \n",
        "Самый простой способ токенизации -- стандартный питоновский __str.split__ метод.  \n",
        "По умолчанию он разбивает текст по последовательностям пробелов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IiGIcl7qIav",
        "outputId": "0485a47d-dc55-4522-b11d-2633cf57c6ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1', '', '2', '3']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'1  2 3'.split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTx9jlFyqIaw",
        "outputId": "77ba82cf-44a4-41ba-95a1-8f4bdb24418f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1', '2', '3']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'1  2 3'.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "969f38NLwtc_"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
        "\n",
        "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
        "\n",
        "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\n",
        "\n",
        "\"\"\"\n",
        "# текст отсюда - https://habr.com/ru/post/582620/м"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Wf2misqIaw",
        "outputId": "c014b83f-9864-4973-e57b-a9b273423d66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['рассмотреть,',\n",
              " 'какие',\n",
              " 'языковые',\n",
              " 'явления',\n",
              " 'данная',\n",
              " 'модель',\n",
              " 'понимает',\n",
              " 'хорошо,',\n",
              " 'а',\n",
              " 'на',\n",
              " 'каких',\n",
              " '–',\n",
              " '\"плывёт\";',\n",
              " 'по',\n",
              " 'этому',\n",
              " 'принципу',\n",
              " 'устроены',\n",
              " 'диагностические',\n",
              " 'датасеты',\n",
              " 'SuperGLUE']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.split()[10:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTfgySybqIaw"
      },
      "source": [
        "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
        "Можно пройтись по всем словам и убрать из них пунктуацию с методом str.strip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m-mwqBpiqIaw"
      },
      "outputs": [],
      "source": [
        "#основные знаки преминания хранятся в питоновском модуле string в punctuation\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "86a0kgapqIaw",
        "outputId": "5b226003-e7cc-48fa-ff77-6dedc92f4f81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nveAs4IwqIaw"
      },
      "outputs": [],
      "source": [
        "# в этом списке не хватает кавычек-ёлочек, лапок, длинного тире и многоточия\n",
        "string.punctuation += '«»—…“”'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3KH9fk7qIax",
        "outputId": "c4b7759c-85d7-493e-9665-101fe67b077b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['рассмотреть',\n",
              " 'какие',\n",
              " 'языковые',\n",
              " 'явления',\n",
              " 'данная',\n",
              " 'модель',\n",
              " 'понимает',\n",
              " 'хорошо',\n",
              " 'а',\n",
              " 'на',\n",
              " 'каких',\n",
              " '–',\n",
              " 'плывёт',\n",
              " 'по',\n",
              " 'этому',\n",
              " 'принципу',\n",
              " 'устроены',\n",
              " 'диагностические',\n",
              " 'датасеты',\n",
              " 'SuperGLUE']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[word.strip(string.punctuation) for word in text.split()][10:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB66BS_LqIax"
      },
      "source": [
        "Так не будут удаляться дефисы и точки в сокращениях, не разделенных пробелом.\n",
        "Метод `strip(chars)` в Python удаляет все вхождения символов, указанных в chars, из начала и конца строки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Im-G7ZKqIax",
        "outputId": "47f7618d-9574-410d-cfd3-71b15ff68081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'как-нибудь'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'как-нибудь'.strip(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RQsHiVDxqIax",
        "outputId": "97c58401-f6c9-4fd3-cab7-5df511d0badf"
      },
      "outputs": [],
      "source": [
        "'т.е.'.strip(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzJhO8FKqIax",
        "outputId": "b0f01e3c-18a2-4e98-df75-7ba50a3b7d92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Пример', 'текста', 'с', 'знаками', 'препинания']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_test = \"Пример текста с, знаками: препинания!\"\n",
        "\n",
        "# Удаляем знаки препинания из середины строки\n",
        "text_test_clean = ''.join(el for el in text_test if el not in string.punctuation)\n",
        "\n",
        "text_test_clean.split(' ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aow5c28VqIax"
      },
      "source": [
        "Например, готовые токенизаторы есть в nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном.\n",
        "\n",
        "**wordpunct_tokenizer** разбирает по регулярке - `'\\w+|[^\\w\\s]+'` (попробуйте понять как она работает просто глядя на паттерн)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPL96JqYqIax"
      },
      "source": [
        "`\\w+:` Эта часть регулярного выражения соответствует одному или более буквенно-цифровым символам (буквы, цифры и знак подчеркивания _). + означает \"один или более\", а \\w соответствует любому буквенно-цифровому символу.\n",
        "\n",
        "`|:` Этот символ в регулярных выражениях означает \"или\". Таким образом, регулярное выражение будет искать соответствие либо \\w+, либо следующей части.\n",
        "\n",
        "`[^\\w\\s]+`: Эта часть регулярного выражения соответствует одному или более символам, которые не являются буквенно-цифровыми (\\w) и не являются пробелами (\\s). `[^\\w\\s]` соответствует любому символу, который не является буквенно-цифровым символом или пробелом. + означает \"один или более\" таких символов.  \n",
        "\n",
        "То есть регулярка ищет либо последовательность буквенно-цифровых символов (слово), либо последовательность символов, которые не являются буквенно-цифровыми и не являются пробелами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xxvAXizKqIay"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIYpWxAeqIay",
        "outputId": "a7fc95d6-0e93-469f-837a-88cd5635d874"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/antontravkin/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/share/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# wordpunct_tokenize(text)[:10]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m130\u001b[39m:\u001b[32m150\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/python_rtk/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/antontravkin/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/share/nltk_data'\n    - '/Users/antontravkin/Sites/python_rtk/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# wordpunct_tokenize(text)[:10]\n",
        "word_tokenize(text)[130:150]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZMOSdfqIay"
      },
      "source": [
        "**word_tokenize** также построен на регулярках, но они там более сложные (учитывается последовательность некоторых\n",
        "символов, символы начала, конца слова и т.д).\n",
        "\n",
        "Специально подобранного под русский язык токенизатора там нет,\n",
        "но и с английским всё работает достаточно хорошо --\n",
        "сокращения вроде \"т.к\" собираются в один токен, дефисные слова тоже не разделяются, многоточия тут тоже не отделяются, но это можно поправить."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT8DECzOqIay",
        "outputId": "edd6cb0c-b4d3-4872-d1c7-2bdc1c6d42ed"
      },
      "outputs": [],
      "source": [
        "word_tokenize(text)[130:150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8FkV0Y4qIay",
        "outputId": "139bf51d-1f21-4d84-accf-fb5dd4284d9f"
      },
      "outputs": [],
      "source": [
        "text1 = \"Hello, World! It's a beautiful day.\"\n",
        "tokens = wordpunct_tokenize(text1)\n",
        "tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aVWYEOSqIay",
        "outputId": "88efa5c8-463d-4878-d8b7-db4874da23aa"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(text1)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYZeynhXqIay"
      },
      "source": [
        "В генсиме тоже есть функция для токенизации(разбивает текст на токены, преобразуя его в нижний регистр, удаляя знаки препинания и разбивая текст на слова. )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zhbdjg5AqIaz"
      },
      "outputs": [],
      "source": [
        "from gensim.utils import tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ9Om2pZqIaz",
        "outputId": "7321fb20-0557-4afe-ddf8-2ddcf5de1cef"
      },
      "outputs": [],
      "source": [
        "list(tokenize(text, lowercase=True))[30:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhITydb7qIaz",
        "outputId": "143a589c-9c69-449e-ccd8-a3e382e0c4d4"
      },
      "outputs": [],
      "source": [
        "list(tokenize(text1, lowercase=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nNCYXzzqIaz"
      },
      "source": [
        "И в razdel тоже есть токенизация  \n",
        "\n",
        "\n",
        "быстрый токенизатор, специально предназначенный для русского языка. Он разбивает текст на токены, учитывая различия в буквенных регистрах и знаках препинания. Токенизация с использованием razdel часто бывает более точной для русского текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5PrtQOOqIaz"
      },
      "outputs": [],
      "source": [
        "from razdel import tokenize as razdel_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRIXMJo0qIaz",
        "outputId": "3963c320-30b0-49c3-f8fd-d72341bd383f"
      },
      "outputs": [],
      "source": [
        "list(razdel_tokenize(text))[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY8tMc_WqIa0",
        "outputId": "40e389dd-bfa8-4593-ef2a-6c258dca1c9e"
      },
      "outputs": [],
      "source": [
        "[token.text for token in list(razdel_tokenize(text))[:10]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1diFwgWqIa0"
      },
      "source": [
        "Работать с регистром тяжело и поэтому можно привести все к нижнему регистру"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVLBmfAKqIa0",
        "outputId": "664b8aa0-6b88-4136-b4fd-ae19f615769b"
      },
      "outputs": [],
      "source": [
        "[token.text.lower() for token in list(razdel_tokenize(text))[:10]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNzegdOKNA7b"
      },
      "source": [
        "задача\n",
        "\n",
        "задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzvBFQZ0NKU2"
      },
      "source": [
        "NLP natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sU2pxTkqIa2"
      },
      "source": [
        "# Нормализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYo3raZeqIa3"
      },
      "source": [
        "В последнее время нормализация (т.е. приведение токенов к стандартному виду) используется все реже. Это связано с использованием subword или byte токенизации в топовых моделях. Однако у них есть свои недостатки и забывать про нормализацию пока не стоит."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asi6JcpjqIa3"
      },
      "source": [
        "Два основных вида нормализации - лемматизация и стемминг. Стемминг уже нигде не используется, но его полезно разобрать, чтобы понимать почему нужно использовать лемматизацию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yup-MYejqIa3"
      },
      "source": [
        "Стемминг - это урезание слова до его \"основы\" (стема), т.е. такой части, которая является общей для всех словоформ в парадигме слова\n",
        "\n",
        "На практике стемминг сводится к отбрасыванию частотных окончаний."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC6TOJFDqIa3"
      },
      "source": [
        "Самый известный стеммер - стеммер Портера (или snowball стеммер).\n",
        "Подробнее про стеммер Портера можно почитать вот тут - <https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340>  \n",
        "А совсем подробнее вот тут - <http://snowball.tartarus.org/algorithms/russian/stemmer.html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQryYJfqqIa3"
      },
      "source": [
        "Готовые стеммеры для разных языков есть в nltk. Работают они вот так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z3L89TwqIa3"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fPJcwfhqIa3"
      },
      "outputs": [],
      "source": [
        "stemmer = SnowballStemmer('russian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBWD_P4bqIa3",
        "outputId": "7ce0be58-2d37-41d4-a9af-aa72ca869217"
      },
      "outputs": [],
      "source": [
        "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxnH7UPiqIa3"
      },
      "source": [
        "Недостатки стемминга достаточно очевидные:  \n",
        "1) с супплетивными формами или редкими окончаниями слова стемминг работать не умеет  \n",
        "2) к одной основе могут приводится разные слова  \n",
        "3) к разным основам могут сводиться формы одного слова  \n",
        "4) приставки не отбрасываются\n",
        "\n",
        "Супплетивные формы - это словоформы, которые в языке образуются не путем добавления приставок или суффиксов, а путем полной замены корня слова.\n",
        " - \"Go\" и \"went\"\n",
        " - \"Is\" и \"am\"\n",
        " - \"Идти\" и \"шёл\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0FTTkZEqIa4"
      },
      "source": [
        "# Лемматизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF9sVFGyqIa4"
      },
      "source": [
        "Лемматизация - это замена словоформы слова в парадигме на какую-то заранее выбранную стадартную форму (лемму).\n",
        "\n",
        "\n",
        "\n",
        "Например, для разных форм глагола леммой обычно является неопределенная форма (инфинитив), а для существительного форма мужского рода единственного числа. Это позволяет избавиться от недостатков стемминга (будет, был - одна лемма), (пролить, пролом - разные). Однако лемматизация значительно сложнее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKAmKaEhqIa4"
      },
      "source": [
        "К счастью есть готовые хорошие лемматизаторы. Для русского основых варианта два:Pymorphy и Mystem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G348ZyZxqIa4"
      },
      "source": [
        "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lre5O63aqIa4"
      },
      "source": [
        "Важным достоинством Mystem является то, что он работает не с отдельными словами, а с целым предложением. При определении нужной леммы учитывается контекст, что позволяет во многих случаях разрешать омонимию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3w8xGciqIa4"
      },
      "source": [
        "### spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr2maLC65rm1",
        "outputId": "c748efb6-e48c-4e3b-e084-ca0caac110fe"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ru_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEhboZbm5Tge",
        "outputId": "3cd3bda2-f6f0-4f70-e9d3-bb5ccac40daa"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Загружаем модель spaCy\n",
        "nlp = spacy.load(\"ru_core_news_sm\")  # Для русского языка, можно заменить на en_core_web_sm для английского\n",
        "\n",
        "text = \"Лучшие бегуны бегали по парку с собакой.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Лемматизация:\", lemmatized_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
